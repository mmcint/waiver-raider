{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mmcint/waiver-raider/blob/main/fantasy_pros_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SWOygyYJkJq4"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import concurrent.futures\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def scrape_fantasypros(position, season):\n",
    "    url = f\"https://www.fantasypros.com/nfl/advanced-stats-{position}.php?year={season}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table', {'id': 'data'})\n",
    "\n",
    "        headers = [th.text for th in table.find_all('th')]\n",
    "        rows = []\n",
    "        for tr in table.find_all('tr')[1:]:\n",
    "            rows.append([td.text for td in tr.find_all('td')])\n",
    "\n",
    "        df = pd.DataFrame(rows, columns=headers)\n",
    "        df['Season'] = season\n",
    "        df['Position'] = position.upper()\n",
    "        df['Player'] = df['Player'].astype(str)\n",
    "        df = df.iloc[1:,:]\n",
    "\n",
    "        if position == 'qb':\n",
    "          url = f\"https://www.fantasypros.com/nfl/stats/qb.php?year={season}\"\n",
    "          response = requests.get(url)\n",
    "          response.raise_for_status()\n",
    "          soup = BeautifulSoup(response.content, 'html.parser')\n",
    "          table = soup.find('table', {'id': 'data'})\n",
    "\n",
    "          headers = [th.text for th in table.find_all('th')]\n",
    "          rows = []\n",
    "          for tr in table.find_all('tr')[1:]:\n",
    "              rows.append([td.text for td in tr.find_all('td')])\n",
    "\n",
    "          rushing_list_df = pd.DataFrame(rows, columns=headers)\n",
    "          rushing_list_df['Season'] = season\n",
    "          rushing_list_df['Position'] = position.upper()\n",
    "          out_df = rushing_list_df.rename(columns={'ATT':'Rush_Att', 'YDS': 'Rush_Yds', 'TD':'Rush_TDs'})\n",
    "          out_df = out_df.drop_duplicates()\n",
    "          merged_df = df.merge(out_df, how='inner', on=['Player', 'Season'])\n",
    "          cols = list(merged_df.loc[:, ~merged_df.columns.isin(['Player', 'Position'])].columns)\n",
    "          merged_df[cols] = merged_df[cols].apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "          merged_df['ADOT'] = np.round(merged_df['AIR']/merged_df['TGT'])\n",
    "          return merged_df\n",
    "        else:\n",
    "          cols = list(df.loc[:, ~df.columns.isin(['Player', 'Position'])].columns)\n",
    "          df[cols] = df[cols].apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "          return df\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Error scraping {position.upper()} data for {season}: {str(e)}\")\n",
    "        return None\n",
    "    except AttributeError as e:\n",
    "        logging.error(f\"Error parsing {position.upper()} data for {season}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def scrape_worker(args):\n",
    "    position, season = args\n",
    "    return scrape_fantasypros(position, season)\n",
    "\n",
    "def save_to_excel(all_data, output_file):\n",
    "    with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
    "        positions = set()\n",
    "        for df in all_data:\n",
    "            if df is not None and 'Position' in df.columns:\n",
    "                positions.update(df['Position'].unique())\n",
    "\n",
    "        for position in positions:\n",
    "            position_data = pd.concat([df for df in all_data if df is not None and 'Position' in df.columns and position in df['Position'].values], ignore_index=True)\n",
    "            if not position_data.empty:\n",
    "                position_data.to_excel(writer, sheet_name=position, index=False)\n",
    "    logging.info(f\"Data saved to {output_file} with separate tabs for each position\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6-sejfT6kepO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-30 07:35:36,148 - ERROR - Error processing QB data for 2023: Columns must be same length as key\n",
      "2024-12-30 07:35:36,520 - ERROR - Error processing RB data for 2022: Columns must be same length as key\n",
      "2024-12-30 07:35:36,890 - ERROR - Error processing RB data for 2023: Columns must be same length as key\n",
      "2024-12-30 07:35:37,611 - ERROR - Error processing QB data for 2022: Columns must be same length as key\n",
      "2024-12-30 07:35:37,612 - WARNING - No data was successfully scraped.\n"
     ]
    }
   ],
   "source": [
    "positions = ['qb', 'rb']\n",
    "seasons = [2022,2023]\n",
    "\n",
    "scrape_args = [(position, season) for position in positions for season in seasons]\n",
    "\n",
    "all_data = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    future_to_args = {executor.submit(scrape_worker, arg): arg for arg in scrape_args}\n",
    "    for future in concurrent.futures.as_completed(future_to_args):\n",
    "        args = future_to_args[future]\n",
    "        try:\n",
    "            df = future.result()\n",
    "            if df is not None:\n",
    "                all_data.append(df)\n",
    "                logging.info(f\"Scraped {args[0].upper()} data for {args[1]}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {args[0].upper()} data for {args[1]}: {str(e)}\")\n",
    "\n",
    "if all_data:\n",
    "    output_file = f\"fantasypros_advanced_stats_{seasons[0]}-{seasons[1]}.xlsx\"\n",
    "    save_to_excel(all_data, output_file)\n",
    "else:\n",
    "    logging.warning(\"No data was successfully scraped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4o_uJ6DmmEtL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO0CNML/RJ1+bfRkcq/8i7Y",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
