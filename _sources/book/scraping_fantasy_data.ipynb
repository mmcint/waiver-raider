{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mmcint/waiver-raider/blob/main/scraping_fantasy_data_10_12_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYpint3V50yb"
   },
   "source": [
    "# Scraping Fantasy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYr3Gbyt0AYu"
   },
   "source": [
    "This is a notebook that will scrape advanced stats for offensive fantasy football positions. The website used to scrape is FantasyPros. There are some additional metrics calculated after the scraping of the data. The intent is for this to be the first of many notebooks shared analyzing key fantasy football metrics. I intend to provide commentary in the notebooks to catelogue my thoughts and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4GDkvvXaLC_f"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import concurrent.futures\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SWOygyYJkJq4"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def scrape_fantasypros(position, season):\n",
    "    url = f\"https://www.fantasypros.com/nfl/advanced-stats-{position}.php?year={season}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table', {'id': 'data'})\n",
    "\n",
    "        headers = [th.text for th in table.find_all('th')]\n",
    "        rows = []\n",
    "        for tr in table.find_all('tr')[1:]:\n",
    "            rows.append([td.text for td in tr.find_all('td')])\n",
    "\n",
    "        df = pd.DataFrame(rows, columns=headers)\n",
    "        df['Season'] = season\n",
    "        df['Position'] = position.upper()\n",
    "        df['Player'] = df['Player'].astype(str)\n",
    "        df = df.iloc[1:,:]\n",
    "\n",
    "        if position == 'qb':\n",
    "          url = f\"https://www.fantasypros.com/nfl/stats/qb.php?year={season}\"\n",
    "          response = requests.get(url)\n",
    "          response.raise_for_status()\n",
    "          soup = BeautifulSoup(response.content, 'html.parser')\n",
    "          table = soup.find('table', {'id': 'data'})\n",
    "\n",
    "          headers = [th.text for th in table.find_all('th')]\n",
    "          rows = []\n",
    "          for tr in table.find_all('tr')[1:]:\n",
    "              rows.append([td.text for td in tr.find_all('td')])\n",
    "\n",
    "          rushing_list_df = pd.DataFrame(rows, columns=headers)\n",
    "          rushing_list_df['Season'] = season\n",
    "          rushing_list_df['Position'] = position.upper()\n",
    "          out_df = rushing_list_df.rename(columns={'ATT':'Rush_Att', 'YDS': 'Rush_Yds', 'TD':'Rush_TDs'})\n",
    "          out_df = out_df.drop_duplicates()\n",
    "          merged_df = df.merge(out_df, how='inner', on=['Player', 'Season'])\n",
    "          cols = list(merged_df.loc[:, ~merged_df.columns.isin(['Player', 'Position'])].columns)\n",
    "          merged_df[cols] = merged_df[cols].apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "          merged_df['ADOT'] = np.round(merged_df['AIR']/merged_df['TGT'])\n",
    "          return merged_df\n",
    "        else:\n",
    "          cols = list(df.loc[:, ~df.columns.isin(['Player', 'Position'])].columns)\n",
    "          df[cols] = df[cols].apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "          return df\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Error scraping {position.upper()} data for {season}: {str(e)}\")\n",
    "        return None\n",
    "    except AttributeError as e:\n",
    "        logging.error(f\"Error parsing {position.upper()} data for {season}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def scrape_worker(args):\n",
    "    position, season = args\n",
    "    return scrape_fantasypros(position, season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6-sejfT6kepO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-30 07:32:18,510 - ERROR - Error processing QB data for 2023: Columns must be same length as key\n"
     ]
    }
   ],
   "source": [
    "positions = ['qb']\n",
    "seasons = [2023]\n",
    "\n",
    "scrape_args = [(position, season) for position in positions for season in seasons]\n",
    "\n",
    "all_data = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    future_to_args = {executor.submit(scrape_worker, arg): arg for arg in scrape_args}\n",
    "    for future in concurrent.futures.as_completed(future_to_args):\n",
    "        args = future_to_args[future]\n",
    "        try:\n",
    "            df = future.result()\n",
    "            if df is not None:\n",
    "                all_data.append(df)\n",
    "                logging.info(f\"Scraped {args[0].upper()} data for {args[1]}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {args[0].upper()} data for {args[1]}: {str(e)}\")\n",
    "\n",
    "# if all_data:\n",
    "#     output_file = f\"fantasypros_advanced_stats_{seasons[0]}-{seasons[1]}.xlsx\"\n",
    "#     save_to_excel(all_data, output_file)\n",
    "# else:\n",
    "#     logging.warning(\"No data was successfully scraped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IDl22-2Qks5D"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPY9Qy0hGc60GtshDFrbONs",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
